{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplified version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List\n",
    "\n",
    "import requests\n",
    "import torch\n",
    "import tiktoken\n",
    "\n",
    "from playground_gpt.embedding import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_complete = requests.get(\n",
    "    \"https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\"\n",
    ").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "embedder = Embedding(input_size=1, embed_size=3, vocab_size=50257)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Your journey starts with one step\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have the tokens for \"Your journey starts with one step\":\n",
    "\n",
    "```\n",
    "Your        -->    7120\n",
    "journey     -->    7002\n",
    "starts      -->    4940\n",
    "with        -->    351\n",
    "one         -->    530\n",
    "step        -->    2239\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7120, 7002, 4940, 351, 530, 2239]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids: List[int] = tokenizer.encode(text)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have the `n=3` embeddings for those tokens:\n",
    "\n",
    "```\n",
    "Your      --->    [1.8105, 1.2218, 1.2664]   \n",
    "journey   --->    [1.3837, 1.3195, 1.8066]   \n",
    "starts    --->    [1.5646, 0.4626, 1.3061]   \n",
    "with      --->    [1.2394, 1.4075, 1.4538]   \n",
    "one       --->    [1.2369, 1.2138, 1.5761]   \n",
    "step      --->    [1.3719, 1.2501, 1.3292]   \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-01-14 15:15:27\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mInputting token_ids as a list. Ideally you would convert it to torch.Tensor before embedding.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.8105, 1.2218, 1.2664],\n",
       "        [1.3837, 1.3195, 1.8066],\n",
       "        [1.5646, 0.4626, 1.3061],\n",
       "        [1.2394, 1.4075, 1.4538],\n",
       "        [1.2369, 1.2138, 1.5761],\n",
       "        [1.3719, 1.2501, 1.3292]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = embedder.positional_embed(token_ids)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to calculate the attention weights\n",
    "\n",
    "$$ w_i = [ X_i \\cdot X_0, X_i \\cdot X_1, \\dots , X_i \\cdot X_n ] $$\n",
    "\n",
    "$\\vec{w}$ is a `NxN` matrix, where N is the number of tokens being inputted.\n",
    "\n",
    "We also need to normalize $\\vec{w}$, and for that we use the softmax function, defined as:\n",
    "\n",
    "$$\\sigma(\\vec{z}_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$$\n",
    "\n",
    "Softmax is used because it makes the results always positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_softmax(vector: torch.Tensor) -> torch.Tensor:\n",
    "    applied_vector = torch.empty(vector.shape[0])\n",
    "    for idx, x_i in enumerate(vector):\n",
    "        applied_vector[idx] = math.exp(x_i)\n",
    "    return applied_vector / applied_vector.sum()\n",
    "\n",
    "\n",
    "def calculate_attention_weight(\n",
    "    query: torch.Tensor, context: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    weights = torch.empty(context.shape[0])\n",
    "    for i, x_i in enumerate(context):\n",
    "        weights[i] = torch.dot(query, x_i)\n",
    "    weights = torch.softmax(\n",
    "        weights, dim=0\n",
    "    )  # torch softmax is better at handling very large/small data\n",
    "    return weights\n",
    "\n",
    "\n",
    "def self_attention(embeddings: torch.Tensor) -> torch.Tensor:\n",
    "    weights = torch.empty((embeddings.shape[0], embeddings.shape[0]))\n",
    "    for idx, elem in enumerate(embeddings):\n",
    "        attention_idx = calculate_attention_weight(elem, embeddings)\n",
    "        weights[idx] = attention_idx\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_self_attention(embeddings: torch.Tensor) -> torch.Tensor:\n",
    "    # the same operation as x_i times x_j in two for loops\n",
    "    weights = embeddings @ embeddings.T\n",
    "    return torch.softmax(weights, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2571, 0.2652, 0.0685, 0.1455, 0.1334, 0.1303],\n",
       "        [0.1929, 0.3227, 0.0542, 0.1569, 0.1511, 0.1223],\n",
       "        [0.2313, 0.2514, 0.1167, 0.1318, 0.1408, 0.1281],\n",
       "        [0.1994, 0.2957, 0.0535, 0.1676, 0.1519, 0.1320],\n",
       "        [0.1939, 0.3017, 0.0606, 0.1611, 0.1539, 0.1288],\n",
       "        [0.2165, 0.2794, 0.0631, 0.1601, 0.1473, 0.1336]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = fast_self_attention(embeddings)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4637, 1.2253, 1.4892],\n",
       "        [1.4296, 1.2436, 1.5268],\n",
       "        [1.4623, 1.1847, 1.4832],\n",
       "        [1.4304, 1.2437, 1.5150],\n",
       "        [1.4301, 1.2376, 1.5178],\n",
       "        [1.4412, 1.2335, 1.5039]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights @ embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
